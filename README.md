Hackman: A Hybrid AI Hangman SolverThis repository contains the solution for the "Hackman" AI challenge, a project to build an intelligent agent that excels at the game of Hangman. The agent uses a hybrid approach, combining a probabilistic n-gram oracle with a Deep Q-Network (DQN) to make strategic, context-aware guesses.The entire project, from data processing to training and evaluation, is contained in the Hackathon_Hangman_AI.ipynb notebook.ğŸš€ The ChallengeThe goal of this hackathon was to build an intelligent Hangman assistant that wins with the fewest possible mistakes. The problem required a hybrid system:Part 1 (The "Oracle"): A probabilistic model (inspired by an HMM) to estimate the probability of each remaining letter.Part 2 (The "Brain"): A Reinforcement Learning (RL) agent that uses the oracle's output to choose the optimal letter to guess.ğŸ¤– System ArchitectureOur solution implements this hybrid design by feeding the oracle's probabilities directly into the state vector of a DQN agent.Part 1: The Probabilistic "HMM" Oracle (HMMOracleNgram)Instead of a formal, state-based HMM, we implemented a powerful pattern-matching oracle that is more direct and accurate for this task.Initialization: On load, the oracle reads the entire data/corpus.txt and groups all 50,000 words into len_buckets (a dictionary based on word length). This makes filtering by length instantaneous.Primary Strategy (Candidate Filtering): When asked for letter probabilities (via letter_prob), the oracle first filters its word buckets to find all candidates that match the current pattern (e.g., _a__e). It then counts the frequency of all unguessed letters in the blank (_) positions of these candidates to generate a highly accurate, context-specific probability distribution.Fallback Strategy (Positional Counts): If the candidate list is empty (e.g., the word is unknown), the oracle falls back to using pre-computed pos_counts. This is a statistical model of the most common letters for each position at a given word length (e.g., for 5-letter words, 's' is very common at the 4th index).Part 2: The DQN "Brain" (DQNAgent)The "brain" is a Deep Q-Network (DQN) agent that learns how to use the oracle's probabilities to make the best decision based on the current game state (lives, guessed letters, etc.).Network (DQNNet): A simple feed-forward neural network with two hidden layers (256 and 128 neurons). The input is the 443-dimension state vector, and the output is a 26-value vector (the "Q-values" for guessing each letter).Learning: The agent uses standard DQN techniques, including:Experience Replay: Storing (state, action, reward, next_state) tuples in a replay buffer to learn from past experiences.Target Network: Using a separate target_net (synced every 1000 steps) to stabilize training.Policy ($\epsilon$-greedy):Exploration: With $\epsilon$ probability, the agent picks a random valid (unguessed) action to discover new strategies.Exploitation: Otherwise, it asks the network for its Q-values and applies an action mask (setting the Q-value of already-guessed letters to $-\infty$) to ensure it only picks the best valid action.The Hybrid State Vector (build_state_vector)The key to this hybrid design is the 443-dimensional state vector fed into the DQN. It's a combination of four distinct pieces of information:Mask Vector (390 values): The current word mask (e.g., _a__e) is one-hot encoded and padded to a max length of 15.Guessed Vector (26 values): A binary vector indicating which of the 26 letters have already been guessed.Lives (1 value): A single normalized float representing the lives remaining (e.g., 5.0 / 6.0).Oracle Probabilities (26 values): The raw probability vector from the HMMOracleNgram.This design allows the DQN to learn a complex policy that weighs the oracle's "suggestion" against the risk (low lives) and the game's history (guessed letters).ğŸ® How to Run1. SetupThe project is designed to run in a Google Colab environment.Upload the Hackathon_Hangman_AI.ipynb notebook and the Data.zip file to your Colab instance.Run Cell 1 to upload Data.zip.Run Cell 2 to unzip the data folder.Run Cell 3 to pip install all necessary dependencies (torch, numpy, matplotlib, tqdm).Run Cells 38 & 39 to fix the file paths and initialize the oracle with the training corpus.2. TrainingRun Cell 25 (# train.py) to start the training loop.This script will instantiate all components (HMMOracleNgram, HangmanEnv, DQNAgent).It will train the agent for 20,000 episodes (games) using words from data/corpus.txt.A tqdm progress bar will show the training progress and the agent's recent average reward.Upon completion, the trained model is saved as dqn_policy.pth, and the training metrics are saved as rewards_per_ep.npy and losses.npy.3. EvaluationRun Cell 64 (# Evaluate the trained model) to test the agent's performance.This script loads the saved dqn_policy.pth model.It loads the 2000-word data/test.txt file for evaluation.It runs 200 games against the test set using a greedy policy (no exploration).It calculates and prints the final score based on the official hackathon formula:Final Score = (Success Rate * 2000) - (Total Wrong Guesses * 5) - (Total Repeated Guesses * 2)ğŸ“Š ResultsThe agent successfully learns to improve its strategy. The smoothed reward graph (plotted by Cell 65) shows a clear upward trend, indicating that the agent got better at winning games and avoiding penalties as training progressed.Final Evaluation Score(Based on 200 games from test.txt after 20,000 training episodes)Success Rate: 0.170 (17.0%)Total Wrong Guesses: 1138Total Repeated Guesses: 0ğŸ Final Score: -5350.00The agent successfully learned the most basic rule: never make a repeated guess (0 repeated guesses). The negative score indicates that more training (>20,000 episodes) and hyperparameter tuning are required to make the agent consistently profitable.ğŸ› ï¸ Technologies UsedPython 3PyTorchNumPyMatplotlibtqdmJupyter / Google Colab
